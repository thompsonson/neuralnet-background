{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environement set up details\n",
    "\n",
    "## Installing locally (rather than colab)\n",
    "\n",
    "Using `uv` (from https://docs.astral.sh/uv/)\n",
    "\n",
    "```\n",
    "# install uv in its own global location (using pipx)\n",
    "pipx install uv\n",
    "# create a virtual environment\n",
    "uv venv\n",
    "# activate the environment\n",
    "source .venv/bin/activate\n",
    "# install the Jupyter notebook packages\n",
    "uv pip install ipykernel jupyter notebook\n",
    "# install required packages\n",
    "uv pip install numpy pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the same iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "\n",
    "df.tail()\n",
    "\n",
    "# Extract the first hundred class labels (it is known that 50 are Iris-setosa and 50 are Iris-virginica)\n",
    "y = df.iloc[0:100, 4].values\n",
    "# update the class labels to -1 or 1\n",
    "y = np.where(y == 'Iris-setosa', -1, 1)\n",
    "# Extract the first hundred features in columns 0 and 1, representively representing sepal length and petal length\n",
    "X = df.iloc[0:100, [0, 2]].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code for the Adaline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineGD(object):\n",
    "    \"\"\"ADAptive LINEar neuron classifier.\n",
    "\n",
    "    Adaline implements a continuous linear activation function (identity)\n",
    "    and uses gradient descent to minimize the cost function, making it\n",
    "    different from the Perceptron which uses a unit step function.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes over the training dataset.\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight\n",
    "      initialization.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after fitting.\n",
    "    cost_ : list\n",
    "      Sum-of-squares cost function value in each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit training data using gradient decent\n",
    "\n",
    "        Unlike the Perceptron, Adaline updates weights based on a continuous\n",
    "        linear activation function rather than a threshold function, which\n",
    "        allows for gradient-based optimization.\n",
    "\n",
    "        Implementation steps:\n",
    "        1. Initialize weights with small random values\n",
    "        2. For each epoch:\n",
    "           a. Calculate the net input (weighted sum via the aggregation function)\n",
    "           b. Apply activation function (in this case it is the identity function)\n",
    "           c. Calculate errors (difference between actual and predicted)\n",
    "           d. Calculate the derivative (gradient) of the cost function wrt the weights and bias\n",
    "           e. Update all weights and biases based on the derivatives multipled by the learning rate\n",
    "           f. Calculate and store cost for this epoch\n",
    "\n",
    "        Note: the deriveative of the cost function wrt the weights is calculated using the chain rule:\n",
    "          ∂E/∂w_j = ∂E/∂φ * ∂φ/∂z * ∂z/∂w_j\n",
    "\n",
    "        where:\n",
    "          ∂E/∂φ = -(y - φ) = (φ - y)  [The actual output (aggregation_function) minus the desired output (y)]\n",
    "          ∂φ/∂z = 1                   [The derivative of the activation (activation_function) wrt the net input (aggregation_function)]\n",
    "          ∂z/∂w_j = x_j               [The derivative of the net input (aggregation_function) wrt the jth input (X_j)]\n",
    "\n",
    "        Here we jump to using numpy in the code.\n",
    "\n",
    "        The important thing to remember is... the Jacobian matrix is the first order derivative of the cost function.\n",
    "\n",
    "        Rather than looping though each input (also called a feature) and updating the weights, we can use matrix multiplication to update all weights at once.\n",
    "\n",
    "        The matrix multiplication is done by taking the dot product of the transpose of the input matrix (X.T) and the error vector (y - self.activation_function(self.aggregation_function(X))).\n",
    "\n",
    "        This reduces to this code:\n",
    "          X.T.dot(X.dot(w) - y) = X.T.dot(y - self.activation_function(self.aggregation_function(X)))\n",
    "\n",
    "        In the for loop below this code is broken out to mirror the steps in the algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_examples, n_features]\n",
    "          Training vectors, where n_examples is the number of examples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_examples]\n",
    "          Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        # Step 1: Initialize weights with small random values\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "        self.cost_ = []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            # Step 2a: Calculate net input (weighted sum via aggregation_function)\n",
    "            net_input = self.aggregation_function(X)\n",
    "\n",
    "            # Step 2b: Apply activation function (in this case it is the identity function)\n",
    "            output = self.activation_function(net_input)\n",
    "\n",
    "            # Step 2c: Calculate prediction error\n",
    "            error_vector = output - y\n",
    "\n",
    "            # Step 2d: Calculate the derivative (gradient) of the cost function wrt the weights and bias\n",
    "            # using the chain rule: ∂E/∂w = X.T.dot(output - y)\n",
    "            derivative_cost_wrt_weights = X.T.dot(error_vector)\n",
    "            derivative_cost_wrt_bias = error_vector.sum()\n",
    "\n",
    "            # Step 2e: Apply standard gradient descent update: w = w - eta * gradient\n",
    "            self.w_[1:] -= self.eta * derivative_cost_wrt_weights\n",
    "            self.w_[0] -= self.eta * derivative_cost_wrt_bias\n",
    "\n",
    "            # Step 2f: Calculate and store cost\n",
    "            cost = (error_vector**2).sum() / 2.0  # Sum of squared errors / 2\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "\n",
    "    def aggregation_function(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def activation_function(self, X):\n",
    "        \"\"\"Compute linear activation\"\"\"\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.activation_function(self.aggregation_function(X)) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The clean version - conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_adaline_analysis(X, y, custom_config=None):\n",
    "    \"\"\"\n",
    "    Run Adaline analysis with a specified configuration\n",
    "\n",
    "    Parameters:\n",
    "    X : array-like, default feature matrix (used only if no input_data in config)\n",
    "    y : array-like, target vector\n",
    "    custom_config : dict, custom configuration (optional)\n",
    "\n",
    "    Returns:\n",
    "    list of trained models\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    import inspect\n",
    "    import re\n",
    "    \n",
    "    # Function to find variable name from calling frame\n",
    "    def get_variable_name(var):\n",
    "        callers_local_vars = inspect.currentframe().f_back.f_back.f_locals.items()\n",
    "        return [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
    "    \n",
    "    # Default configuration\n",
    "    default_config = {\n",
    "        \"input_data\": [X, X, X],  # Default to using the input X\n",
    "        \"learning_rates\": [0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
    "        \"n_iters\": [10, 15, 20, 25, 30, 35],\n",
    "        \"markers\": ['o', 'x', 'x', 'o', '+', '+']\n",
    "    }\n",
    "\n",
    "    # Use custom config if provided, otherwise use default\n",
    "    config = custom_config if custom_config is not None else default_config\n",
    "\n",
    "    # Extract configuration values\n",
    "    input_data_list = config[\"input_data\"]\n",
    "    learning_rates = config[\"learning_rates\"]\n",
    "    n_iters = config[\"n_iters\"]\n",
    "    markers = config.get(\"markers\", ['o'] * len(learning_rates))\n",
    "\n",
    "    # Get the number of data sets and experiments per dataset\n",
    "    n_data_sets = len(input_data_list)\n",
    "    n_experiments = len(learning_rates)  # All learning rates used for each dataset\n",
    "    \n",
    "    # Create a figure with n_data_sets*2 rows, n_experiments columns\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=2*n_data_sets, \n",
    "        ncols=n_experiments, \n",
    "        figsize=(3*n_experiments, 4*n_data_sets)\n",
    "    )\n",
    "    \n",
    "    # Handle the case when there's only one row or column\n",
    "    if n_data_sets == 1 and n_experiments == 1:\n",
    "        axes = np.array([[axes[0]], [axes[1]]])\n",
    "    elif n_data_sets == 1:\n",
    "        axes = axes.reshape(2, n_experiments)\n",
    "    elif n_experiments == 1:\n",
    "        axes = axes.reshape(2*n_data_sets, 1)\n",
    "    \n",
    "    # Train models and plot results\n",
    "    models = []\n",
    "    \n",
    "    for data_idx, X_data in enumerate(input_data_list):\n",
    "        # Calculate row indices for this data set\n",
    "        row_learning = data_idx * 2\n",
    "        row_decision = data_idx * 2 + 1\n",
    "        \n",
    "        # Handle different input formats\n",
    "        if isinstance(X_data, tuple) and len(X_data) == 2:\n",
    "            X, data_label = X_data\n",
    "        else:\n",
    "            X = X_data\n",
    "            # Try to get the variable name\n",
    "            var_names = get_variable_name(X_data)\n",
    "            if var_names:\n",
    "                data_label = f\"Dataset {var_names[0]}\"\n",
    "            else:\n",
    "                data_label = f\"Dataset {data_idx+1}\"\n",
    "            \n",
    "        # Add a title for this data set\n",
    "        if axes.ndim == 2 and axes.shape[1] > 0:\n",
    "            axes[row_learning, 0].text(-0.3, 0.5, data_label, \n",
    "                                      rotation=90, \n",
    "                                      transform=axes[row_learning, 0].transAxes,\n",
    "                                      ha='center', va='center',\n",
    "                                      fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Train and plot for each parameter setting\n",
    "        for i in range(n_experiments):\n",
    "            # Get parameters for this experiment - same learning rates for each dataset\n",
    "            eta = learning_rates[i]\n",
    "            n_iter = n_iters[i] if i < len(n_iters) else n_iters[-1]  # Use last n_iter if not enough provided\n",
    "            marker = markers[i] if i < len(markers) else 'o'  # Use 'o' as default if not enough markers\n",
    "            \n",
    "            # Train model\n",
    "            model = AdalineGD(n_iter=n_iter, eta=eta).fit(X, y)\n",
    "            models.append(model)\n",
    "            \n",
    "            # Row 1: Learning curves\n",
    "            ax_learning = axes[row_learning, i]\n",
    "            \n",
    "            if i < 2:\n",
    "                ax_learning.plot(range(1, len(model.cost_) + 1), np.log10(model.cost_), marker=marker)\n",
    "                ax_learning.set_ylabel('log(SSE)')\n",
    "            else:\n",
    "                ax_learning.plot(range(1, len(model.cost_) + 1), model.cost_, marker=marker)\n",
    "                ax_learning.set_ylabel('SSE')\n",
    "\n",
    "            ax_learning.set_xlabel('Epochs')\n",
    "            ax_learning.set_title(f'eta {eta}')\n",
    "\n",
    "            # Row 2: Decision boundaries\n",
    "            ax_decision = axes[row_decision, i]\n",
    "            plot_decision_regions(X, y, classifier=model, resolution=0.02, ax=ax_decision)\n",
    "            \n",
    "            # Add legend to last plot in each row\n",
    "            if i == n_experiments - 1:\n",
    "                ax_decision.legend()\n",
    "\n",
    "            ax_decision.set_xlabel('sepal length')\n",
    "            ax_decision.set_ylabel('petal length')\n",
    "            ax_decision.set_title(f'Decision Boundary (eta={eta})')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return models\n",
    "\n",
    "# Modify plot_decision_regions to accept an axis parameter\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02, ax=None):\n",
    "    \"\"\"\n",
    "    Plot decision regions for a classifier in a 2D feature space.\n",
    "\n",
    "    Parameters:\n",
    "    X : array-like, shape = [n_samples, n_features]\n",
    "        Feature matrix.\n",
    "    y : array-like, shape = [n_samples]\n",
    "        Target vector.\n",
    "    classifier : object\n",
    "        Trained classifier with a predict method.\n",
    "    resolution : float, optional (default=0.02)\n",
    "        Resolution of the mesh grid used to plot the decision surface.\n",
    "    ax : matplotlib axis, optional\n",
    "        Axis to plot on. If None, uses current axis.\n",
    "\n",
    "    This function visualizes the decision boundaries of a classifier by plotting\n",
    "    the decision surface in a 2D feature space.\n",
    "    \"\"\"\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # If ax is None, get current axis\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Calculate grid of the features\n",
    "    feature_1_min, feature_1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    feature_2_min, feature_2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    # Create a fine grid of points\n",
    "    feature_1_values, feature_2_values = np.meshgrid(\n",
    "        np.arange(feature_1_min, feature_1_max, resolution),\n",
    "        np.arange(feature_2_min, feature_2_max, resolution)\n",
    "    )\n",
    "\n",
    "    # Flatten and transpose the grid for prediction\n",
    "    feature_grid = np.array([feature_1_values.ravel(), feature_2_values.ravel()]).T\n",
    "\n",
    "    # Use the classifier to predict labels for the grid\n",
    "    label_per_point = classifier.predict(feature_grid)\n",
    "    label_per_point = label_per_point.reshape(feature_1_values.shape)\n",
    "\n",
    "    # Plot the decision surface\n",
    "    ax.contourf(feature_1_values, feature_2_values, label_per_point,\n",
    "               alpha=0.4, cmap=cmap)\n",
    "\n",
    "    # Set plot limits\n",
    "    ax.set_xlim(feature_1_values.min(), feature_1_values.max())\n",
    "    ax.set_ylim(feature_2_values.min(), feature_2_values.max())\n",
    "\n",
    "    # Plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        ax.scatter(\n",
    "            x=X[y == cl, 0],\n",
    "            y=X[y == cl, 1],\n",
    "            alpha=0.8,\n",
    "            c=[cmap(idx)],\n",
    "            marker=markers[idx],\n",
    "            label=cl,\n",
    "            edgecolors='black'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basline the training of data as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_config = {\n",
    "    \"input_data\": [X],  # Array of different datasets to try\n",
    "    \"learning_rates\": [0.001, 0.0006, 0.0005, 0.0002, 0.00009],\n",
    "    \"n_iters\": [10, 10, 40, 500, 1000],\n",
    "    \"markers\": ['o', 'x', 'o', '+', '+']\n",
    "}\n",
    "\n",
    "\n",
    "# Run the analysis\n",
    "models = run_adaline_analysis(X, y, custom_config=baseline_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize features\n",
    "\n",
    "X_std = np.copy(X)\n",
    "X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()\n",
    "X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()\n",
    "\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot original data\n",
    "ax1.scatter(X[:, 0], X[:, 1])\n",
    "ax1.set_title('Original Data')\n",
    "ax1.set_xlabel('sepal length')\n",
    "ax1.set_ylabel('petal length')\n",
    "\n",
    "# Plot standardized data\n",
    "ax2.scatter(X_std[:, 0], X_std[:, 1])\n",
    "ax2.set_title('Standardized Data')\n",
    "ax2.set_xlabel('sepal length [standardized]')\n",
    "ax2.set_ylabel('petal length [standardized]')\n",
    "\n",
    "# Add a grid for better visualization\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print some statistics to understand the transformation\n",
    "print(\"\\nOriginal Data Statistics:\")\n",
    "print(f\"Feature 1 - Mean: {X[:, 0].mean():.2f}, Std: {X[:, 0].std():.2f}\")\n",
    "print(f\"Feature 2 - Mean: {X[:, 1].mean():.2f}, Std: {X[:, 1].std():.2f}\")\n",
    "\n",
    "print(\"\\nStandardized Data Statistics:\")\n",
    "print(f\"Feature 1 - Mean: {X_std[:, 0].mean():.2f}, Std: {X_std[:, 0].std():.2f}\")\n",
    "print(f\"Feature 2 - Mean: {X_std[:, 1].mean():.2f}, Std: {X_std[:, 1].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "normalised_config = {\n",
    "    \"input_data\": [X, X_std],  # Array of different datasets to try\n",
    "    \"learning_rates\": [0.1, 0.01, 0.001, 0.0006, 0.0005, 0.0002],\n",
    "    \"n_iters\": [100, 100, 100, 100, 100],\n",
    "    \"markers\": ['o', 'x', 'o', '+', '+']\n",
    "}\n",
    "\n",
    "\n",
    "models = run_adaline_analysis(X_std, y, custom_config=normalised_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_config = {\n",
    "    \"learning_rates\": [0.001, 0.0006, 0.0005, 0.0002, 0.00009],\n",
    "    \"n_iters\": [10, 10, 40, 500, 1000],\n",
    "    \"markers\": ['o', 'x', 'o', '+', '+']\n",
    "}\n",
    "\n",
    "\n",
    "# Run the analysis\n",
    "models = run_adaline_analysis(X, y, custom_config=baseline_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
