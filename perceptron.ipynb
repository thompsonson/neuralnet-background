{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environement set up details\n",
    "\n",
    "## Installing locally (rather than colab)\n",
    "\n",
    "Using `uv` (from https://docs.astral.sh/uv/)\n",
    "\n",
    "```\n",
    "# install uv in its own global location (using pipx)\n",
    "pipx install uv\n",
    "# create a virtual environment\n",
    "uv venv\n",
    "# activate the environment\n",
    "source .venv/bin/activate\n",
    "# install the Jupyter notebook packages\n",
    "uv pip install ipykernel jupyter notebook\n",
    "# install required packages\n",
    "uv pip install numpy pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graph of a Perceptron\n",
    "\n",
    "![Computation Graph of a Perceptron](images/perceptron/perceptron.png)\n",
    "\n",
    "The character 𝚫 (Delta) represents a change or update in the weight w_k. This represents the adjustment made to the weights during the training process.\n",
    "\n",
    "Where:\n",
    "- x_k: The input feature corresponding to the weight w_k.\n",
    "- w_k: The weight w_k of the perceptron.\n",
    "- y is the known desired  output.\n",
    "- ŷ (y_hat) is the predicted output. \n",
    "- 𝚫w_k: The change in the weight w_k.\n",
    "- Ƞ (eta): The learning rate, which controls the size of the weight updates.\n",
    "- error: The difference between the predicted value (ŷ) and the desired  value (y).\n",
    "\n",
    "Components:\n",
    "- Aggregate function: a summation of the outputs of x . w. \n",
    "- Threshold function: sets the predicted value (ŷ) to 1 or -1 depending on the output of the aggregation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code comes from Sebastian Raschka's Python Machine Learning book\n",
    "\n",
    "\n",
    "class Perceptron(object):\n",
    "    \"\"\"Perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "        Passes over the training dataset\n",
    "\n",
    "    Attributes\n",
    "    ------------\n",
    "    w_ : 1d-array\n",
    "        Weights after fitting\n",
    "    errors_ : list\n",
    "        Number of misclassifications in every epoch\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta=0.01, n_iter=50):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Training function that implements the perceptron learning rule.\n",
    "\n",
    "        1. Initialize weights to zero\n",
    "    \n",
    "        For each epoch:\n",
    "\n",
    "            2. For each sample:\n",
    "                - Make prediction\n",
    "                - Calculate error\n",
    "                - Update weights if prediction is wrong\n",
    "            3. Track number of errors per epoch\n",
    "        \n",
    "        4. return the instance of the Perceptron object\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        ------------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.w_ = np.zeros(1 + X.shape[1])\n",
    "        self.errors_ = []\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_[1:] += update * xi\n",
    "                self.w_[0] += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"\n",
    "        The Aggregation Function - Calculates the weighted sum of inputs (dot product).\n",
    "        Formula: z = w0 + w1x1 + w2x2 + ... + wnxn\n",
    "        where w0 is the bias unit and w1...wn are the weights\n",
    "        \"\"\"\n",
    "        # uses numpy's dot method to calculate the dot product of the inputs X and the weights w_\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        The Threshold function - Implements the unit step function (threshold).\n",
    "        Returns:\n",
    "        1 if net input >= 0\n",
    "        -1 if net input < 0\n",
    "        \"\"\"\n",
    "        # uses numpy's where method to return 1 if net_input is greater than or equal to 0, otherwise -1\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a perceptron model on the Iris dataset\n",
    "\n",
    "doanload the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract and plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first hundred class labels (it is known that 50 are Iris-setosa and 50 are Iris-virginica)\n",
    "y = df.iloc[0:100, 4].values\n",
    "# update the class labels to -1 or 1\n",
    "y = np.where(y == 'Iris-setosa', -1, 1)\n",
    "# Extract the first hundred features in columns 0 and 1, representively representing sepal length and petal length \n",
    "X = df.iloc[0:100, [0, 2]].values\n",
    "\n",
    "# now plot the data\n",
    "plt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label='setosa')\n",
    "plt.scatter(X[50:100, 0], X[50:100, 1], color='blue', marker='x', label='versicolor')\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: this is clearly a linearly seperable problem\n",
    "\n",
    "now we train the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppn = Perceptron(eta=0.1, n_iter=10)\n",
    "ppn.fit(X, y)\n",
    "\n",
    "# plot the errors per epoch\n",
    "\n",
    "plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of misclassifications')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron converged after 6 epochs\n",
    "\n",
    "Let's plot the decision boundry now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original plotting of the decision boundary\n",
    "\n",
    "I include this code for completeness, it is directly from the book and may mean something for you. However I did not understand it immediately. \n",
    "\n",
    "I have been on a short journey to understand that the function is creating a grid of feature values and then running each value through the Perceptron's predict method to return the label for that point on a grid.\n",
    "\n",
    "This creates a data set of 3 dimensions (petal length, sepal length, and label). The function uses a contour plot to represent the 3rd dimenstion (the label) as a colour.\n",
    "\n",
    "The methods used to do this (specifically `arange`, `meshgrid`, and `ravel`) are from numpy and are new to me. I took some time to practical understand what they are doing, I include that journey after the original code for my reference and to help others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot the inputs (X), the labels (y), \n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "    \"\"\"\n",
    "    Plot decision regions for a classifier in a 2D feature space.\n",
    "\n",
    "    Parameters:\n",
    "    X : array-like, shape = [n_samples, n_features]\n",
    "        Feature matrix.\n",
    "    y : array-like, shape = [n_samples]\n",
    "        Target vector.\n",
    "    classifier : object\n",
    "        Trained classifier with a predict method.\n",
    "    resolution : float, optional (default=0.02)\n",
    "        Resolution of the mesh grid used to plot the decision surface.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function visualizes the decision boundaries of a classifier by plotting\n",
    "    the decision surface and the data points in a 2D feature space. Different\n",
    "    markers and colors are used for different classes.\n",
    "    \"\"\"\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    x1_range = np.arange(x1_min, x1_max, resolution)\n",
    "    x2_range = np.arange(x2_min, x2_max, resolution)\n",
    "\n",
    "    # Generate the meshgrid using np.meshgrid\n",
    "    xx1, xx2 = np.meshgrid(x1_range, x2_range)\n",
    "\n",
    "    # \n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "\n",
    "    # use a filled contour plot to represent the 3d data (features petal (y) and sepal (x) lengths plus predicted class (z))\n",
    "    # in a 2d plot. in this plot the z axis becomes the colours. \n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, c=[cmap(idx)],\n",
    "                    marker=markers[idx], label=cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, classifier=ppn)\n",
    "\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('petal length (cm)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the numpy methods that plot the decision boundary\n",
    "\n",
    "Below we can see that arange is creating an array from the first parameter to the second parameter, each value is the value before incremented by the third parameter until it arrives at the end.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate a range of values from 0 to 10 with a step size of 1\n",
    "range_1 = np.arange(0, 10, 1)\n",
    "print(\"Range with step size 1:\", range_1)\n",
    "\n",
    "# Generate a range of values from 0 to 10 with a step size of 0.5\n",
    "range_2 = np.arange(0, 10, 0.5)\n",
    "print(\"Range with step size 0.5:\", range_2)\n",
    "\n",
    "# Generate a range of values from 0 to 10 with a step size of 2\n",
    "range_3 = np.arange(0, 10, 2)\n",
    "print(\"Range with step size 2:\", range_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the min and max values for the ranges\n",
    "feature_1_min, feature_1_max = 1, 5\n",
    "feature_2_min, feature_2_max = 10, 20\n",
    "resolution = 1\n",
    "\n",
    "# Generate the ranges using np.arange\n",
    "feature_1_range = np.arange(feature_1_min, feature_1_max, resolution)\n",
    "feature_2_range = np.arange(feature_2_min, feature_2_max, resolution)\n",
    "\n",
    "print(\"feature_1_range:\", feature_1_range)\n",
    "print(\"feature_2_range:\", feature_2_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the ranges for each feature we can conbine them to make a grid. \n",
    "\n",
    "Remember the point of this is to create a contour (like a map) of the label value for each point on a feature grid.\n",
    "\n",
    "Numpy has a feature called `meshgird` that is used to create two arrays of the feature values for each point on the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the meshgrid using np.meshgrid\n",
    "feature_1_values, feature_2_values = np.meshgrid(feature_1_range, feature_2_range)\n",
    "\n",
    "print(\"feature_1_values:\\n\", feature_1_values)\n",
    "print(\"feature_2_values:\\n\", feature_2_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualise the grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the meshgrid\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(feature_1_values, feature_2_values, s=50, color='blue')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Meshgrid Visualization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to flatten the values and then combine them to into an array that represents the feature grid points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ravel to flatten the meshgrid arrays\n",
    "feature_1_ravel = feature_1_values.ravel()\n",
    "feature_2_ravel = feature_2_values.ravel()\n",
    "\n",
    "# Print the flattened arrays\n",
    "print(\"xx1_ravel:\", feature_1_ravel)\n",
    "print(\"xx2_ravel:\", feature_2_ravel)\n",
    "\n",
    "# Combine the flattened arrays and transpose\n",
    "feature_grid_points = np.array([feature_1_ravel, feature_2_ravel]).T\n",
    "\n",
    "# Print the combined and transposed array\n",
    "print(\"grid_points:\\n\", feature_grid_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive function to plot the decision boundary\n",
    "\n",
    "Now that the different numpy methods have been tested on more vanilla data it should be clear. If it isn't change the original values for these\n",
    "\n",
    "```\n",
    "# Define the min and max values for the ranges\n",
    "feature_1_min, feature_1_max = 1, 5\n",
    "feature_2_min, feature_2_max = 10, 20\n",
    "resolution = 1\n",
    "```\n",
    "\n",
    "By changing these it is possible to get a clearer view of the feature gird that the numpy methods are creating. \n",
    "\n",
    "Next the use of the classifier is renamed. This was important to me as, initially I'd thought that the function was doing the classification for the inputs (i.e. X). However X is only being used to get the minimum and maximum for the features. \n",
    "\n",
    "If this isn't clear, it may be worth changing `plot_decision_regions` to accept the min and max of the features rather than the whole data range.\n",
    "\n",
    "In going over this bit of code:\n",
    "\n",
    "```python\n",
    "\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "```\n",
    "\n",
    "the varaibles have been changed to these names, highlighting that the classifier is being used to values for the feature grid (therefore not for X) \n",
    "\n",
    "```python\n",
    "    \n",
    "    # use the classifier to calculate the label at each point on the gird\n",
    "    label_per_point_on_feature_grid = classifier.predict(feature_grid)\n",
    "    # Reshape the predictions back to match our grid dimensions for plotting\n",
    "    label_per_point_on_feature_grid = label_per_point_on_feature_grid.reshape(feature_1_values.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "    \"\"\"\n",
    "    Plot decision regions for a classifier in a 2D feature space.\n",
    "\n",
    "    Parameters:\n",
    "    X : array-like, shape = [n_samples, n_features]\n",
    "        Feature matrix.\n",
    "    y : array-like, shape = [n_samples]\n",
    "        Target vector.\n",
    "    classifier : object\n",
    "        Trained classifier with a predict method.\n",
    "    resolution : float, optional (default=0.02)\n",
    "        Resolution of the mesh grid used to plot the decision surface.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function visualizes the decision boundaries of a classifier by plotting\n",
    "    the decision surface in a 2D feature space.\n",
    "    \"\"\"\n",
    "    # setup marker generator and color map\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    # calculate a grid of the features (feature_1 is the sepal length and feature_2 is the petal length)\n",
    "    \n",
    "    # firstly get the min and max of each feature\n",
    "    feature_1_min, feature_1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    feature_2_min, feature_2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    # Create a fine grid of points covering our feature space so we can visualize\n",
    "    # how the perceptron classifies every possible combination of feature values\n",
    "    feature_1_values, feature_2_values = np.meshgrid(np.arange(feature_1_min, feature_1_max, resolution),\n",
    "                                                     np.arange(feature_2_min, feature_2_max, resolution))\n",
    "    # now flatten the 2 dimensional arrays into a one dimension array for both features\n",
    "    # the array is transposed so that each point in the grid is represented as a row\n",
    "    feature_grid = np.array([feature_1_values.ravel(), feature_2_values.ravel()]).T\n",
    "    \n",
    "    # use the classifier to calculate the label at each point on the gird\n",
    "    label_per_point_on_feature_grid = classifier.predict(feature_grid)\n",
    "    # Reshape the predictions back to match our grid dimensions for plotting\n",
    "    label_per_point_on_feature_grid = label_per_point_on_feature_grid.reshape(feature_1_values.shape)\n",
    "\n",
    "    # plot the decision surface\n",
    "\n",
    "    # Create a filled contour plot where different colors show the different predicted classes\n",
    "    # alpha=0.4 makes the coloring semi-transparent so we can see the data points\n",
    "    plt.contourf(feature_1_values, feature_2_values, label_per_point_on_feature_grid, alpha=0.4, cmap=cmap)\n",
    "    # Set the plot limits to show the full decision boundary region\n",
    "    plt.xlim(feature_1_values.min(), feature_1_values.max())\n",
    "    plt.ylim(feature_2_values.min(), feature_2_values.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further higlight that this function is just for plotting the decision boundary the plotting of the features (X) and tehir corresponding labels (y) has been moved to a seperate function called `plot_class_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_samples(X, y, label_names):\n",
    "    \"\"\"\n",
    "    Plot class samples in a 2D feature space.\n",
    "\n",
    "    Parameters:\n",
    "    X : array-like, shape = [n_samples, n_features]\n",
    "        Feature matrix.\n",
    "    y : array-like, shape = [n_samples]\n",
    "        Target vector.\n",
    "    label_names : list\n",
    "        List of label names corresponding to the unique classes in y.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function plots the class samples in a 2D feature space with a legend.\n",
    "    \"\"\"\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, c=[cmap(idx)],\n",
    "                    marker=markers[idx], label=label_names[idx])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brining it all together we are able to plot the decision boundary and the samples, I have added the axis labels as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, classifier=ppn)\n",
    "label_names = ['setosa', 'versicolor']\n",
    "plot_class_samples(X, y, label_names)\n",
    "# Label the axes with the feature names to show what we're plotting\n",
    "plt.xlabel('Sepal length [cm]')\n",
    "plt.ylabel('Petal length [cm]')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
