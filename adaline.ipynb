{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environement set up details\n",
    "\n",
    "## Installing locally (rather than colab)\n",
    "\n",
    "Using `uv` (from https://docs.astral.sh/uv/)\n",
    "\n",
    "```\n",
    "# install uv in its own global location (using pipx)\n",
    "pipx install uv\n",
    "# create a virtual environment\n",
    "uv venv\n",
    "# activate the environment\n",
    "source .venv/bin/activate\n",
    "# install the Jupyter notebook packages\n",
    "uv pip install ipykernel jupyter notebook\n",
    "# install required packages\n",
    "uv pip install numpy pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the same iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "\n",
    "df.tail()\n",
    "\n",
    "# Extract the first hundred class labels (it is known that 50 are Iris-setosa and 50 are Iris-virginica)\n",
    "y = df.iloc[0:100, 4].values\n",
    "# update the class labels to -1 or 1\n",
    "y = np.where(y == 'Iris-setosa', -1, 1)\n",
    "# Extract the first hundred features in columns 0 and 1, representively representing sepal length and petal length\n",
    "X = df.iloc[0:100, [0, 2]].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code for the Adaline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineGD(object):\n",
    "    \"\"\"ADAptive LINEar neuron classifier.\n",
    "\n",
    "    Adaline implements a continuous linear activation function (identity)\n",
    "    and uses gradient descent to minimize the cost function, making it\n",
    "    different from the Perceptron which uses a unit step function.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes over the training dataset.\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight\n",
    "      initialization.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after fitting.\n",
    "    cost_ : list\n",
    "      Sum-of-squares cost function value in each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit training data using gradient decent\n",
    "\n",
    "        Unlike the Perceptron, Adaline updates weights based on a continuous\n",
    "        linear activation function rather than a threshold function, which\n",
    "        allows for gradient-based optimization.\n",
    "\n",
    "        Implementation steps:\n",
    "        1. Initialize weights with small random values\n",
    "        2. For each epoch:\n",
    "           a. Calculate the net input (weighted sum via the aggregation function)\n",
    "           b. Apply activation function (in this case it is the identity function)\n",
    "           c. Calculate errors (difference between actual and predicted)\n",
    "           d. Calculate the derivative (gradient) of the cost function wrt the weights and bias\n",
    "           e. Update all weights and biases based on the derivatives multipled by the learning rate\n",
    "           f. Calculate and store cost for this epoch\n",
    "\n",
    "        Note: the deriveative of the cost function wrt the weights is calculated using the chain rule:\n",
    "          ∂E/∂w_j = ∂E/∂φ * ∂φ/∂z * ∂z/∂w_j\n",
    "\n",
    "        where:\n",
    "          ∂E/∂φ = -(y - φ) = (φ - y)  [The actual output (aggregation_function) minus the desired output (y)]\n",
    "          ∂φ/∂z = 1                   [The derivative of the activation (activation_function) wrt the net input (aggregation_function)]\n",
    "          ∂z/∂w_j = x_j               [The derivative of the net input (aggregation_function) wrt the jth input (X_j)]\n",
    "\n",
    "        Here we jump to using numpy in the code.\n",
    "\n",
    "        The important thing to remember is... the Jacobian matrix is the first order derivative of the cost function.\n",
    "\n",
    "        Rather than looping though each input (also called a feature) and updating the weights, we can use matrix multiplication to update all weights at once.\n",
    "\n",
    "        The matrix multiplication is done by taking the dot product of the transpose of the input matrix (X.T) and the error vector (y - self.activation_function(self.aggregation_function(X))).\n",
    "\n",
    "        This reduces to this code:\n",
    "          X.T.dot(X.dot(w) - y) = X.T.dot(y - self.activation_function(self.aggregation_function(X)))\n",
    "\n",
    "        In the for loop below this code is broken out to mirror the steps in the algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_examples, n_features]\n",
    "          Training vectors, where n_examples is the number of examples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_examples]\n",
    "          Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        # Step 1: Initialize weights with small random values\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "        self.cost_ = []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            # Step 2a: Calculate net input (weighted sum via aggregation_function)\n",
    "            net_input = self.aggregation_function(X)\n",
    "\n",
    "            # Step 2b: Apply activation function (in this case it is the identity function)\n",
    "            output = self.activation_function(net_input)\n",
    "\n",
    "            # Step 2c: Calculate prediction error\n",
    "            error_vector = output - y\n",
    "\n",
    "            # Step 2d: Calculate the derivative (gradient) of the cost function wrt the weights and bias\n",
    "            # using the chain rule: ∂E/∂w = X.T.dot(output - y)\n",
    "            derivative_cost_wrt_weights = X.T.dot(error_vector)\n",
    "            derivative_cost_wrt_bias = error_vector.sum()\n",
    "\n",
    "            # Step 2e: Apply standard gradient descent update: w = w - eta * gradient\n",
    "            self.w_[1:] -= self.eta * derivative_cost_wrt_weights\n",
    "            self.w_[0] -= self.eta * derivative_cost_wrt_bias\n",
    "\n",
    "            # Step 2f: Calculate and store cost\n",
    "            cost = (error_vector**2).sum() / 2.0  # Sum of squared errors / 2\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "\n",
    "    def aggregation_function(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def activation_function(self, X):\n",
    "        \"\"\"Compute linear activation\"\"\"\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.activation_function(self.aggregation_function(X)) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different learning rates\n",
    "\n",
    "It was interesting to see the effect of different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(10, 4))\n",
    "\n",
    "ada1 = AdalineGD(n_iter=10, eta=0.001).fit(X, y)\n",
    "ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker='o')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('log(SSE)')\n",
    "ax[0].set_title('eta 0.001')\n",
    "\n",
    "\n",
    "ada5 = AdalineGD(n_iter=10, eta=0.0006).fit(X, y)\n",
    "ax[1].plot(range(1, len(ada5.cost_) + 1), np.log10(ada5.cost_), marker='x')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('log(SSE)')\n",
    "ax[1].set_title('eta 0.0006')\n",
    "\n",
    "\n",
    "ada3 = AdalineGD(n_iter=40, eta=0.0005).fit(X, y)\n",
    "ax[2].plot(range(1, len(ada3.cost_) + 1), ada3.cost_, marker='x')\n",
    "ax[2].set_xlabel('Epochs')\n",
    "ax[2].set_ylabel('SSE')\n",
    "ax[2].set_title('eta 0.0005')\n",
    "\n",
    "\n",
    "ada2 = AdalineGD(n_iter=500, eta=0.0002).fit(X, y)\n",
    "ax[3].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker='o')\n",
    "ax[3].set_xlabel('Epochs')\n",
    "ax[3].set_ylabel('SSE')\n",
    "ax[3].set_title('eta 0.0002')\n",
    "\n",
    "\n",
    "ada4 = AdalineGD(n_iter=1000, eta=0.00009).fit(X, y)\n",
    "ax[4].plot(range(1, len(ada4.cost_) + 1), ada4.cost_, marker='+')\n",
    "ax[4].set_xlabel('Epochs')\n",
    "ax[4].set_ylabel('SSE')\n",
    "ax[4].set_title('eta 0.000009')\n",
    "\n",
    "# plt.savefig('images/02_11.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first two graphs have logarithmic y-axis, we can see that the cost (equivalent to the error rate in the Preceptron) increases and doesn't come back down. Interestingly the second does start learn, however it overshots the optimal value.\n",
    "\n",
    "The third graph (note it does not have a logrthmic y-axis) shows a learning rate ot 0.0005 approaches a optimal value but goes past it.\n",
    "\n",
    "The last two graphs show settings of 0.0002 and 0.00009, both converge on the optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The messy version - investigation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "    \"\"\"\n",
    "    Plot decision regions for a classifier in a 2D feature space.\n",
    "\n",
    "    Parameters:\n",
    "    X : array-like, shape = [n_samples, n_features]\n",
    "        Feature matrix.\n",
    "    y : array-like, shape = [n_samples]\n",
    "        Target vector.\n",
    "    classifier : object\n",
    "        Trained classifier with a predict method.\n",
    "    resolution : float, optional (default=0.02)\n",
    "        Resolution of the mesh grid used to plot the decision surface.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function visualizes the decision boundaries of a classifier by plotting\n",
    "    the decision surface in a 2D feature space.\n",
    "    \"\"\"\n",
    "    # setup marker generator and color map\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # calculate a grid of the features (feature_1 is the sepal length and feature_2 is the petal length)\n",
    "\n",
    "    # firstly get the min and max of each feature\n",
    "    feature_1_min, feature_1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    feature_2_min, feature_2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    # Create a fine grid of points covering our feature space so we can visualize\n",
    "    # how the perceptron classifies every possible combination of feature values\n",
    "    feature_1_values, feature_2_values = np.meshgrid(np.arange(feature_1_min, feature_1_max, resolution),\n",
    "                                                     np.arange(feature_2_min, feature_2_max, resolution))\n",
    "    # now flatten the 2 dimensional arrays into a one dimension array for both features\n",
    "    # the array is transposed so that each point in the grid is represented as a row\n",
    "    feature_grid = np.array([feature_1_values.ravel(), feature_2_values.ravel()]).T\n",
    "\n",
    "    # use the classifier to calculate the label at each point on the gird\n",
    "    label_per_point_on_feature_grid = classifier.predict(feature_grid)\n",
    "    # Reshape the predictions back to match our grid dimensions for plotting\n",
    "    label_per_point_on_feature_grid = label_per_point_on_feature_grid.reshape(feature_1_values.shape)\n",
    "\n",
    "    # plot the decision surface\n",
    "\n",
    "    # Create a filled contour plot where different colors show the different predicted classes\n",
    "    # alpha=0.4 makes the coloring semi-transparent so we can see the data points\n",
    "    plt.contourf(feature_1_values, feature_2_values, label_per_point_on_feature_grid, alpha=0.4, cmap=cmap)\n",
    "    # Set the plot limits to show the full decision boundary region\n",
    "    plt.xlim(feature_1_values.min(), feature_1_values.max())\n",
    "    plt.ylim(feature_2_values.min(), feature_2_values.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_samples(X, y, label_names):\n",
    "    \"\"\"\n",
    "    Plot class samples in a 2D feature space.\n",
    "\n",
    "    Parameters:\n",
    "    X : array-like, shape = [n_samples, n_features]\n",
    "        Feature matrix.\n",
    "    y : array-like, shape = [n_samples]\n",
    "        Target vector.\n",
    "    label_names : list\n",
    "        List of label names corresponding to the unique classes in y.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function plots the class samples in a 2D feature space with a legend.\n",
    "    \"\"\"\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, c=[cmap(idx)],\n",
    "                    marker=markers[idx], label=label_names[idx])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ada_gd = AdalineGD(n_iter=35, eta=0.01)\n",
    "# ada_gd.fit(X, y)\n",
    "\n",
    "ada_gd = AdalineGD(n_iter=25, eta=0.0005).fit(X, y)\n",
    "\n",
    "plot_decision_regions(X, y, classifier=ada_gd)\n",
    "label_names = ['setosa', 'versicolor']\n",
    "plot_class_samples(X, y, label_names)\n",
    "\n",
    "plt.title('Adaline - Gradient Descent')\n",
    "plt.xlabel('sepal length [standardized]')\n",
    "plt.ylabel('petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/02_14_1.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, len(ada_gd.cost_) + 1), ada_gd.cost_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/02_14_2.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ada_gd = AdalineGD(n_iter=35, eta=0.01)\n",
    "# ada_gd.fit(X, y)\n",
    "\n",
    "ada_gd = AdalineGD(n_iter=20, eta=0.0005).fit(X, y)\n",
    "\n",
    "plot_decision_regions(X, y, classifier=ada_gd)\n",
    "label_names = ['setosa', 'versicolor']\n",
    "plot_class_samples(X, y, label_names)\n",
    "\n",
    "plt.title('Adaline - Gradient Descent')\n",
    "plt.xlabel('sepal length [standardized]')\n",
    "plt.ylabel('petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/02_14_1.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, len(ada_gd.cost_) + 1), ada_gd.cost_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/02_14_2.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ada_gd = AdalineGD(n_iter=35, eta=0.01)\n",
    "# ada_gd.fit(X, y)\n",
    "\n",
    "ada_gd = AdalineGD(n_iter=35, eta=0.0005).fit(X, y)\n",
    "\n",
    "plot_decision_regions(X, y, classifier=ada_gd)\n",
    "label_names = ['setosa', 'versicolor']\n",
    "plot_class_samples(X, y, label_names)\n",
    "\n",
    "plt.title('Adaline - Gradient Descent')\n",
    "plt.xlabel('sepal length [standardized]')\n",
    "plt.ylabel('petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/02_14_1.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, len(ada_gd.cost_) + 1), ada_gd.cost_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/02_14_2.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ada_gd = AdalineGD(n_iter=35, eta=0.01)\n",
    "# ada_gd.fit(X, y)\n",
    "\n",
    "ada_gd = AdalineGD(n_iter=50, eta=0.0005).fit(X, y)\n",
    "\n",
    "plot_decision_regions(X, y, classifier=ada_gd)\n",
    "label_names = ['setosa', 'versicolor']\n",
    "plot_class_samples(X, y, label_names)\n",
    "\n",
    "plt.title('Adaline - Gradient Descent')\n",
    "plt.xlabel('sepal length [standardized]')\n",
    "plt.ylabel('petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/02_14_1.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, len(ada_gd.cost_) + 1), ada_gd.cost_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/02_14_2.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The clean version - conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_adaline_analysis(X, y, custom_config=None):\n",
    "    \"\"\"\n",
    "    Run Adaline analysis with a specified configuration\n",
    "\n",
    "    Parameters:\n",
    "    X : array-like, feature matrix\n",
    "    y : array-like, target vector\n",
    "    custom_config : dict, custom configuration (optional)\n",
    "\n",
    "    Returns:\n",
    "    list of trained models\n",
    "    \"\"\"\n",
    "    # Default configuration\n",
    "    default_config = {\n",
    "        \"learning_rates\": [0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
    "        \"n_iters\": [10, 15, 20, 25, 30, 35],\n",
    "        \"markers\": ['o', 'x', 'x', 'o', '+', '+']\n",
    "    }\n",
    "\n",
    "    # Use custom config if provided, otherwise use default\n",
    "    config = custom_config if custom_config is not None else default_config\n",
    "\n",
    "    # Extract configuration values\n",
    "    learning_rates = config[\"learning_rates\"]\n",
    "    n_iters = config[\"n_iters\"]\n",
    "    markers = config.get(\"markers\", ['o'] * len(learning_rates))\n",
    "\n",
    "    # Create a figure with 2 rows, N columns\n",
    "    n_experiments = len(learning_rates)\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=n_experiments, figsize=(3*n_experiments, 8))\n",
    "\n",
    "    # Train models and plot results\n",
    "    models = []\n",
    "    for i, (eta, n_iter, marker) in enumerate(zip(learning_rates, n_iters, markers)):\n",
    "        # Train model\n",
    "        model = AdalineGD(n_iter=n_iter, eta=eta).fit(X, y)\n",
    "        models.append(model)\n",
    "\n",
    "        # Row 1: Learning curves\n",
    "        if i < 2:\n",
    "            axes[0, i].plot(range(1, len(model.cost_) + 1), np.log10(model.cost_), marker=marker)\n",
    "            axes[0, i].set_ylabel('log(SSE)')\n",
    "        else:\n",
    "            axes[0, i].plot(range(1, len(model.cost_) + 1), model.cost_, marker=marker)\n",
    "            axes[0, i].set_ylabel('SSE')\n",
    "\n",
    "        axes[0, i].set_xlabel('Epochs')\n",
    "        axes[0, i].set_title(f'eta {eta}')\n",
    "\n",
    "        # Row 2: Decision boundaries\n",
    "        # Plot decision boundary in the second row\n",
    "        plot_decision_regions(X, y, classifier=model, resolution=0.02, ax=axes[1, i])\n",
    "\n",
    "        # Add labels to last subplot in second row\n",
    "        if i == 4:\n",
    "            axes[1, i].legend(['setosa', 'versicolor'])\n",
    "\n",
    "        axes[1, i].set_xlabel('sepal length')\n",
    "        axes[1, i].set_ylabel('petal length')\n",
    "        axes[1, i].set_title(f'Decision Boundary (eta={eta})')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return models\n",
    "\n",
    "# Modify plot_decision_regions to accept an axis parameter\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02, ax=None):\n",
    "    \"\"\"\n",
    "    Plot decision regions for a classifier in a 2D feature space.\n",
    "\n",
    "    Parameters:\n",
    "    X : array-like, shape = [n_samples, n_features]\n",
    "        Feature matrix.\n",
    "    y : array-like, shape = [n_samples]\n",
    "        Target vector.\n",
    "    classifier : object\n",
    "        Trained classifier with a predict method.\n",
    "    resolution : float, optional (default=0.02)\n",
    "        Resolution of the mesh grid used to plot the decision surface.\n",
    "    ax : matplotlib axis, optional\n",
    "        Axis to plot on. If None, uses current axis.\n",
    "\n",
    "    This function visualizes the decision boundaries of a classifier by plotting\n",
    "    the decision surface in a 2D feature space.\n",
    "    \"\"\"\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # If ax is None, get current axis\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Calculate grid of the features\n",
    "    feature_1_min, feature_1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    feature_2_min, feature_2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    # Create a fine grid of points\n",
    "    feature_1_values, feature_2_values = np.meshgrid(\n",
    "        np.arange(feature_1_min, feature_1_max, resolution),\n",
    "        np.arange(feature_2_min, feature_2_max, resolution)\n",
    "    )\n",
    "\n",
    "    # Flatten and transpose the grid for prediction\n",
    "    feature_grid = np.array([feature_1_values.ravel(), feature_2_values.ravel()]).T\n",
    "\n",
    "    # Use the classifier to predict labels for the grid\n",
    "    label_per_point = classifier.predict(feature_grid)\n",
    "    label_per_point = label_per_point.reshape(feature_1_values.shape)\n",
    "\n",
    "    # Plot the decision surface\n",
    "    ax.contourf(feature_1_values, feature_2_values, label_per_point,\n",
    "               alpha=0.4, cmap=cmap)\n",
    "\n",
    "    # Set plot limits\n",
    "    ax.set_xlim(feature_1_values.min(), feature_1_values.max())\n",
    "    ax.set_ylim(feature_2_values.min(), feature_2_values.max())\n",
    "\n",
    "    # Plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        ax.scatter(\n",
    "            x=X[y == cl, 0],\n",
    "            y=X[y == cl, 1],\n",
    "            alpha=0.8,\n",
    "            c=[cmap(idx)],\n",
    "            marker=markers[idx],\n",
    "            label=cl,\n",
    "            edgecolors='black'\n",
    "        )\n",
    "\n",
    "# Run the analysis\n",
    "models = run_adaline_analysis(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"learning_rates\": [0.001, 0.0006, 0.0005, 0.0002, 0.00009],\n",
    "    \"n_iters\": [10, 10, 40, 500, 1000],\n",
    "    \"markers\": ['o', 'x', 'o', '+', '+']\n",
    "}\n",
    "\n",
    "\n",
    "# Run the analysis\n",
    "models = run_adaline_analysis(X, y, custom_config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"learning_rates\": [0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
    "    \"n_iters\": [5, 7, 8, 40, 50, 100],\n",
    "    \"markers\": ['o', 'x', 'o', '+', '+', '+']\n",
    "}\n",
    "\n",
    "\n",
    "# Run the analysis\n",
    "models = run_adaline_analysis(X, y, custom_config=config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
